Related to file `dercuano-rendering`, file `dercuano-formula-display`,
and file `dercuano-drawings`, but not the same — I want some kind of
equation-and-data-plotting thing in Dercuano, with some kind of
Jupyter-like rapid feedback.  I think I can make it simpler and less
fiddly than Numpy or Pandas, using the ideas in file `typed-apl`, file
`principled-apl`, file `relational-modeling-and-apl`, and file
`micro-math-plus`, plus some I got from Darius Bacon, and get some
formula display out of it into the bargain.

As discussed in those notes, prerendering images to PNG or JPEG files
like the humans normally do is not really an option for Dercuano
because of its 5MB total download size budget.

Rapid feedback HCI
------------------

In 2015 I wrote [an RPN
editor](http://canonical.org/~kragen/sw/dev3/rpn-edit) with numbers
and 1-D arrays that generates simple plots and formula renderings as you
calculate, a followon to an [older JS calculator I wrote in
2005](http://canonical.org/~kragen/sw/dev3/js-calc); in 2016 I did [a
similar hack](http://canonical.org/~kragen/81hacks/autodiffgraph)
where instead of calculating on *arrays* the elements you calculate
on are *functions*, starting from the identity function *f*(*x*) = *x*
and constant functions *f<sub>k</sub>*(*x*) = *k*, then combining them
pointwise.  These are all fairly keyboard-driven (file
`interactive-calculator` explores how to do a multitouch UI) and
prototype-quality.

One of the interesting things about the 2015 RPN editor above is that
it uses the URL #fragment identifier to store the entire application
state, much like erlehmann’s `glitch:` URLs for bytebeat, so that you
can bookmark the calculation state or pass it to someone else in a
link.  In some sense, it’s an interactive *viewer and editor* for a
calculation *text*, with some linguistic representation — in this
case, RPN, since nothing more complex is needed.

The 2015 RPN editor also allows you to highlight subexpressions (with
the ←→ keys) to see their values, and to structure-edit it (with
^←/^→), although that is confusing.

Another thing about all three of these prototypes is that you don’t
have to request for a result to be plotted — as soon as it exists, it
gets plotted.  But they need more flexibility in *how* to plot things.
(The 2005 one gives you the option of resizing a plot with the mouse,
while the others don’t even do that much.)  Every value has an
infinity of possible visible presentations; peremptorily displaying
two of them is not enough.

These all feel much more immediate than the experience with
IPython/Jupyter, where you are constantly faced with the alternative
between *using* a value you have calculated:

    t = dt * arange(20e-3 / dt)

and *seeing* it:

    dt * arange(20e-3 / dt)

and plotting a function so you can see *both its domain and range* and
have it *labeled* and have *more than one plot* requires bending over
backwards:

    subplot(211)
    plot(t, VR, label='$V_R$')
    plot(t, VL, label='$V_L$')
    plot(t, VC, label='$V_C$')
    legend()
    subplot(212)
    plot(t, I, label='$I$')
    legend();

Consider, instead, being able to say:

    (VR over VL over VC) atop I

or the equivalent with keystroke or touch commands?  I mean `VR` isn’t
dependent *just* on `t` — [in this notebook it also depends on `C`,
`L`, `R`, and
`dt`](https://nbviewer.jupyter.org/url/canonical.org/%7Ekragen/sw/dev3/curve-tracer.ipynb) — but
`t` is the axis I’ve been thinking of it as varying with here, while
I’ve been treating those other variables as constants.  So is it too
much to ask that my calculating and plotting system would be able to
infer that, at least unless I override it?  Especially when I’m
plotting `VL` *on the same axis where I already plotted `VR` against
`t`*?  Sheesh!

Another thing is that, if you’re evaluating a function of more than
one variable at many points so you can plot it, Numpy (like APL,
Octave, and R) can’t keep straight which variations belong to the
X-axis and which belong to the Y-axis.  It chokes on this:

    R = array([1000, 2200, 4700, 10e3, 22e3, 47e3])
    C = array([100e-9, 220e-9, 470e-9])
    matshow(R * C)

It complains, “ValueError: operands could not be broadcast together
with shapes (6,) (3,)”, which is to say that it was trying to multiply
corresponding elements of `R` and `C` to get time constants.  If we
want the two to vary independently, `R.T * C` doesn’t work as you
might expect, but we can say

    matshow(multiply.outer(R, C))

or

    matshow(R.reshape((6, 1)) * C)

But then the next time you do a calculation involving both `R` and
`C`, you have to tell Numpy *again* that you want them to vary
independently.  And this is what file `principled-apl` is about.
(Also, don’t forget `colorbar()`, which is not the same as
`legend()`.)  This is actually the same problem as getting the X-axis
labels right by default: for Numpy, `R` is just a vector of six
numbers, just as earlier `VR` was an array of 100'000 numbers.  It
doesn’t have any idea why there are six.

The only software I’ve seen that does get this right is μMath+; see
file `micro-math-plus` for details.

Of course, I keep using Jupyter, despite the above, and even though I
can’t incorporate the plots into Dercuano.  That’s because in my 2015
prototype calculator I haven’t even implemented typing in negative
numbers or decimals, much less multidimensional arrays, the Fast
Fourier Transform, or singular value decomposition; moreover, I can
probably expect a 10× slowdown just from switching the inner loops of
these numerical algorithms to JS from Fortran in LAPACK.

Integration with my current workflow
------------------------------------

This AJAXy thing I described above has a difficulty: I’m mostly
writing Dercuano in Emacs, not in some kind of browser-based IDE.  I
could reasonably pop out of writing to the browser to do some graphing
(I could even add a keybinding in Emacs), but ultimately whatever I
put together in the browser needs to be something I can paste into a
text editor, and ideally something that will diff reasonably well.

Probably the best I can hope for there is to pop open a textarea that
says something like

    <script>
    calc(`jasiodj jiaji aoj ioaj iojgosjo
    jaiogjaoj
    aijgwj jaiojgioawj owj oiajio jaweoj jaiojo
    jaiogjwojao ioj ioaj oij ioawj oaj aj iawjiawejisjga0 auj
    ajigwaj jawjiawjipwuj0aweuj890ejgp aji
    ajiajijwijapgjawpj`)
    </script>

where the text inside the ``` `` ``` encodes the calculations and
plotting options.  Then I can copy and paste this into the text
editor, hopefully remembering to delete the previous version.  A
hassle, but manageable.  (Maybe a keybinding can find the surrounding
`<script>` tag, paste in whatever is on the clipboard, and if it looks
like a new `<script>` tag, delete the old one.)

The ``` `` ``` syntax is new in recent versions of JS.

Even the hairiest plots I’ve been doing so far should be encodable in
a kilobyte or two of text, and maybe different plots in the same
document could talk to each other.

Rendering improvement
---------------------

As explained in file `antialiased-line-drawing`, we could go a long
way to improve the readability of graphs by using LCD subpixel
antialiasing and a bit of signal-processing theory, instead of drawing
PostScript-style convolutions of the graph line with a one-dimensional
boxcar kernel at right angles to it.

Ideally, the line plotted on a plot is infinitely thin, a line-shaped
Dirac delta, but rendering it that way requires not only infinite
resolution but also infinite dynamic range.  Bandlimiting the Dirac
delta to a sinc that won’t alias at the screen resolution and maybe
inverse-filtering a bit to compensate for the rectangularity of LCD
pixels (which amounts to a low-pass filter through convolution with a
rectangle) should give a high-quality rendering; windowing the sinc
should make it more computationally tractable, but of course requires
a little more frequency headroom.  Reducing the height of the peak in
the center of the filter kernel should help at reducing the demands on
the dynamic range of screen pixels, but maintaining the sharpness of
the peak there should help with visibility.  High-pass filtering the
filter kernel a bit, maybe without terribly strong stopband
attenuation, should also improve the
precision/visibility/dynamic-range tradeoff.

Attenuating the lowest- and highest-frequency components this way has
the effect of spreading the line’s brightness over more pixels, which
means that it can vary more within the same dynamic range; this is
important when lines cross or pass very nearby.  However, I don’t know
whether the dynamic range increases proportional to the number of
pixels or to their square root.

[Windytan’s oscilloscope-emulation
algorithms](http://www.windytan.com/2013/03/rendering-pcm-with-simulated-phosphor.html)
demonstrate what can be achieved with closer-to-ideal plot
rendering — aside from the issues of correct interpolation close to
the Nyquist frequency, there’s lots of detail that is lost to the
nonlinearities of the standard approach to waveform plotting but
visible on an analog oscilloscope.

It might be possible to get such effects purely in SVG — [SVG 1.1 in
2003][0] already defined the `filter` element and the `filter`
property, which supports an `feConvolveMatrix` filtering primitive
that I think could in theory handle this.  I’ve rarely or never seen
this element in the wild, making me think its implementation is
probably not well tested, and so might have performance or even
correctness issues.  The spec page is well worth reading as an
overview of what 2-D graphical primitives the experts at Adobe thought
were important in 2003; they go well beyond what PostScript can do.

[0]: https://www.w3.org/TR/2003/REC-SVG11-20030114/filters.html

It’s often observed that bright lines on a dark screen background are
more visible to the humans than dark lines on a bright screen
background; this is particularly a problem for things like visualizing
two-dimensional scalar fields such as the signed response of a filter
kernel.  I don’t have a good understanding of why this is; I wonder if
it has something to do with the humans’ logarithmic brightness
perception, where a bit of blurriness diminishes the white around a
black line by an imperceptibly tiny amount, while the same blurriness
will convert the black around a white line into a slightly dimmer
white.

If this is the reason, it means there’s an unavoidable compromise
between correct in-focus appearance (where the logarithmic perception
law means we should do our convolutions in logarithmic color space)
and correct out-of-focus appearance (where the defocus inside the
human’s eye mixes the light linearly, so we should do our convolutions
in linear color space).  Using strong contrast sparingly should reduce
the costs of this compromise.

With these tricks, it should be feasible to get lines that are an
order of magnitude more visible than the traditional 250-micron-wide
125-micron-quantization-noise Bresenham lines that Gnuplot will give
you by default, while at the same time being more than an order of
magnitude more precisely positioned in the X dimension (say, 10 μm),
on a traditional 100-dpi, 250-micron-resolution LCD screen with
vertical RGB subpixels, and nearly a factor of magnitude more
precisely positioned in the Y dimension (say, 30 μm).

On the high-dpi screens now common on hand computers — 200 dpi, or
127-micron pitch with 42-micron pitch if it has RGB subpixels, is a
typical resolution nowadays — it should be possible to get positioning
errors on the order of 5 μm in X and 15 μm in Y.

Still, none of this is needed for an “MVP”, which can be done
straightforwardly with `<canvas>` or SVG (possibly using d3).

What I use most in Numpy, SciPy, and matplotlib
-----------------------------------------------

Maybe if my calculating/plotting thing can do most of the things I can
do in IPython/Jupyter, it’ll be comfortable to use for a variety of
things.

I looked through 16 of my recent IPython notebooks and came up with
this top-64 list by frequency of use (in source code, not execution):

        105 plot
         68 subplot
         55 *
         41 **
         40 len
         40 []
         39 [:]
         35 -
         27 xlim
         27 abs
         26 copy
         19 @
         18 matshow
         17 contour
         17 [:,:]
         16 sum
         16 '.'
         15 set_*scale('log')
         15 resize
         15 print
         15 arange
         15 /
         14 linspace
         14 legend
         14 -=
         13 max
         13 fft.fft
         13 +=
         13 [:,]
         12 stem
         12 colorbar
         12 array([])
         11 zeros
         11 ylim
         10 pi
         10 [,:]=
          9 exp
          9 +
          9 [,:]
          9 >
          8 inv
          8 concatenate
          8 [:]=
          7 []=
          7 [:,:]=
          6 .T
          6 cumsum(axis=)
          6 '.-'
          5 sin
          5 shape
          5 reshape
          5 min
          5 max(axis=)
          5 gca().set_aspect('equal')
          5 cumsum
          5 cond
          5 [:,]=
          4 xticks
          4 where
          4 svd
          4 plot(linewidth=)
          4 [,]
          3 sum(axis=)
          3 round

This is from a bit over 1000 invocations of Numpy array operations and
matplotlib operations.  `plot` is super popular, and so is damned
`subplot`, but `stem`, `matshow`, and `contour` also appear a lot.
Arithmetic `*`, `**`, `-`, `@` (matrix multiply), `/`, `-=`, and `+=`
are very popular; `+` is less so.  Popular aggregate operations are
`len`, `sum`, `max`, and to a lesser extent `min`.  And `abs`, `exp`,
and `sin` are surprisingly popular.

Then there are indexing and slicing operations.  A *lot* of indexing
and slicing operations.  Like, just scalar index reads are #6, more
popular than *subtraction*.  It might have been worthwhile to break
down the kinds of slicing a bit more: sometimes it’s between two
constant indices like `x[200:400]`, sometimes it’s dropping some
elements from the beginning `x[3:]` or the end `x[:-3]`, and sometimes
it’s some other calculated index like `x[pos:pos+size]`.  Sometimes
it’s a coordinate shift, sometimes I intended to select a subset
(often for plotting), etc.

Popular plotting options include `xlim`, `'.'`, `'.-'`,
`yscale('log')` (and occasionally `xscale` too), `legend`, `colorbar`,
`ylim`, `gca().set_aspect('equal')` (which doesn’t have a convenient
function in pyplot the way `set_yscale('log')` does), and `xticks`.

Popular heavy-duty algorithms are `fft.fft`, `inv`, `cond`, and `svd`.
Maybe matrix multiply `@`/`dot` should be included there too.

Popular ways of generating arrays, other than arithmetic, include
`copy`, `resize` (which in Numpy repeats an array, like `tile`),
`arange`, `linspace`, `array([])` (converting a literal list to an
array), `zeros` (typically followed by assignments), and
`concatenate`, which puts the elements of one after the elements of
the other.

Other miscellaneous facilities I apparently use a lot include `pi`,
`cumsum`, `.T`, `reshape` (a generalization of `.T`), and `where`
(conditional: `where(a, b, c)` is `b` where `a` is true, `c` where `a`
is false).

Not all of these operations would map over to other environments in
exactly the same way.  In particular, a lot of the plotting options
are maybe things to set with the mouse.

Attaching aesthetics to data
----------------------------

_The Grammar of Graphics_ refers to the visual appearances we attach
to data to make it visible as “aesthetics” — as in:

> Aesthetic attribute functions are used in two ways.  Most commonly,
> we specify a variable or blend of variables that constitutes a
> dimension, such as *size*(**population**) or
> *color*(**trial1+trial2**).  Or we may assign a constant, such as
> *size*(3) or *color*(“red”).

They specifically disclaim “the derivative modern meanings [of
“aesthetics”] of beauty, taste, and artistic criteria”.

In GG, as in most graphics systems, data do not have aesthetics.
Instead, aesthetics have data.  This is also how matplotlib, d3, and
Gnuplot do things.  The data are floating around in vectors or
whatever, and at some point they collide with a plotting command or a
plot-update command, and at that point they get used, perhaps
ephemerally, to generate a graphic; but subsequently they lose their
connection to the graphic.

I think this is probably not the best approach for an interactive
calculator with instant feedback.  Instead, aesthetics and indeed a
whole presentation should be attached to the data, so that the data
can always be plotted in a sensible way at any point in the
calculation.  (Bret Victor has demonstrated some visualizations of Dan
Amelang’s Nile which probably inspired this thought.)

I don’t know how exactly this should work.  Probably if you plot two
different voltages in different colors or different linewidths, they
should retain those aesthetics whether you’re plotting them against
time or against their common current — but what if you are plotting
them against each other, with one on X and the other on Y?  What if
the current has its own color?  What color should the sum of the
voltages be, or the square of one of them?  I probably need to try
stuff to see what feels least frustrating.

For short discrete signals, `stem` is probably the correct
presentation under most circumstances, and plenty of operations on
discrete signals are closed; so probably if you add two stem-displayed
signals, or multiply one by a constant, you should get another one.
But `stem` becomes unwieldy for sufficiently many samples.  Do I need
conditional formatting?

(One potential benefit of the more symbolic way I’m thinking about
doing things is that discrete and continuous signals are not the
same.)

Square aspect ratios — a common tweak — are nearly always appropriate
when the axes are in the same dimension.  But tagging every variable
with units of measurement might be unwieldy.  (On the other hand, it
might help to associate some aesthetics with units of measurement
rather than values.  And `units.dat`, now `definitions.units`, gzips
to 78 kilobytes.)

The implicit, conditional associations in file `principled-apl` should
help somewhat with the problem of associating varying quantities with
an aesthetic — it should be just as easy to set the voltage's
linewidth to be the current as to set it to 3.  (You might need some
kind of scale mapping from amperes to pixels, though.)

Performance
-----------

JS is not going to be as fast as Fortran, as evidenced by things like
PDF.js, modern JS interpreters can be coaxed to be fast enough to do
some substantial computation.

Firefox takes about 3.6 seconds to run this JS on my laptop:

    function tri(n) {
      let t = 0
      for (let i = 0; i < n; i++) t += i;
      return t;
    }

    tri(1000000000)

It also gets the wrong answer, because of 64-bit floating-point
roundoff error, but that’s not the point.  The point is that it was
able to chew through 280 million loop iterations per second.  Given a
32-millisecond budget to render a graphic, it can do 9 million simple
arithmetic operations like the above.

I tried it in C:

    #include <stdio.h>
    #include <stdlib.h>

    long long tri(long long n)
    {
      long long t = 0;
      for (long long i = 0; i < n; i++) t += i;
      return t;
    }

    int main(int argc, char **argv)
    {
      printf("%lld\n", tri(strtoll(argv[1], 0, 10)));
      return 0;
    }

Without optimization, it was the same speed as Firefox; with
optimization, I had to make the number a parameter to keep GCC from
evaluating the loop at compile time, and it takes 900 ms, four times
as fast.  (Also, it gets the right answer, unlike JS.)

So the cost of JS for this simple integer numerical code is about a
factor of 4.  So JS on my laptop or my phone is faster than C on my
netbook.
