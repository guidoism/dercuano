Some thoughts on SDF raymarching
================================

After writing file `interval-raymarching.md`, I just wrote [my first
raymarcher with signed distance functions (“SDFs”)][0], intentionally
a pretty minimal affair.  It was a lot quicker to write than [My Very
First Raytracer][1], taking about an hour and a page of Lua to
initially get working, rather than all night and four pages of C.
Some of that difference is being able to build it on top of Yeso, so
it doesn’t have to include code for image file output; some of it is
that Lua is a bit terser than C; some is that the raymarcher doesn’t
support color; half a page of it is the input file parsing in the C
raytracer; but most of the reason is that a minimal SDF raymarcher is
simpler than a minimal Whitted-style raytracer.

Disappointingly, although it does manage to do full-motion video, it’s
a great deal slower than the precise raytracer, although I haven’t
added specular reflections, lighting, color, or texture to it yet.  I
don’t yet know if that’s because it’s an iterative approximation
method or because LuaJIT is producing somewhat suboptimal code.  In
this very simple scene, it’s only doing an average of around 9.5 SDF
evaluations per pixel, which is comparable to the number of
intersection tests the precise raytracer needed per ray; this weighs
on the side of blaming LuaJIT.

Of course it’s a bit silly to be rendering real-time 3-D graphics on
the CPU in 2019, even if your GPU *is* just a shitty little Intel Gen8
(see file `asus-gpu`), but it’s still enough experience to provoke me
to write a bunch of things.

[0]: https://gitlab.com/kragen/bubbleos/blob/master/tree/yeso/sdf.lua
[1]: http://canonical.org/~kragen/sw/aspmisc/my-very-first-raytracer

Rounding corners and inexact SDFs
---------------------------------

In iq/rgba’s notes on the topic, he frequently mentions the importance
of using exact SDFs, which provide you with the exact distance to the
nearest scene object, rather than inexact SDFs (I think the original
sphere-tracing paper calls these “distance underestimate functions”),
which merely provide you with a lower bound.  He explains that this is
important because using correct Euclidean SDFs will speed up the
raymarching process dramatically.

Now, this may be true, but unfortunately many of the attractive
modeling features of implicit surfaces involve transforming functions
in ways that do not preserve SDFs’ exactness.  In particular, union
(minimum of SDFs) is exact, but intersection and difference (maximum
of SDFs and of an SDF and a negated SDF) are not exact.  The SDFs
generated by other very appealing features, like surface
perturbations, bounding volume hierarchies (including IFS rendering),
smooth union, and twisting space, are also usually inexact.

More unfortunately, other attractive modeling features of SDFs — such
as the ability to round corners by subtracting a constant, or the
ability to compute a constant-thickness shell by taking the absolute
value and subtracting a constant — depend on the SDFs’ exactness.
Consider a sphere whose SDF is λ*p⃗*.|*p⃗* - *c⃗*<sub>0</sub>| -
*r*<sub>0</sub> and another whose SDF is λ*p⃗*.|*p⃗* - *c⃗*<sub>1</sub>|
- *r*<sub>1</sub>.  Each of these is exact.  If we take the CSG
difference as λ*p⃗*.|*p⃗* - *c⃗*<sub>0</sub>| - *r*<sub>0</sub> ∨
*r*<sub>1</sub> - |*p⃗* - *c⃗*<sub>1</sub>| (where *x* ∨ *y* is the
maximum of *x* and *y*) such that the first sphere has a
second-sphere-shaped bite taken out of it, we get an inexact,
lower-bound SDF.  It’s straightforward to see that attempting to round
the sharp corner where the cut penetrates the surface by adding a
constant will not work; addition distributes over maximum and minimum,
so (*a* - *k*<sub>0</sub> ∨ *k*<sub>1</sub> - *b*) + *r* = *a* -
(*k*<sub>0</sub> - *r*) ∨ (*k*<sub>1</sub> + *r*) - *b*.  That is, all
the level sets of this SDF can be reached by increasing one radius
while decreasing the other.  So the hoped-for rounded edge at the
intersection line never appears.

(I say “it’s straightforward” but it surprised me when I saw it on the
screen.)

A brute-force solution to this problem is to resample the
inexact-SDF-generated geometry into some kind of numerical
approximation that admits efficient exact SDF computation.

Automatic differentiation
-------------------------

I mentioned autodiff briefly in file `interval-raymarching.md`, but
now that I’ve actually written an SDF raytracer, I feel like I have a
better understanding of the situation.

With SDFs you don’t easily know which object you’ve hit.  So if you
want to do something that depends on the surface normal, such as
Lambertian lighting, you don’t have a straightforward way to find it.
Moreover, even if you do have a way to find which underlying primitive
you’ve collided with, its SDF *d*(*p⃗*) doesn’t immediately tell you
how to find its normal.  You need the *gradient* of the SDF,
∇*d*(*p⃗*).  Autodiff can give you that, at the cost of only doubling
the computational cost, rather than quadrupling it, which is the cost
of the standard approach.

However, as I said in file `interval-raymarching.md`, using affine
arithmetic might actually work better than the instantaneous
derivative in this case, in order to avoid aliasing artifacts.
iq/rgba has shown compelling demonstrations of how using a too-small
box to estimate the gradient can cause aliasing.

(If your final top-level SDF is a tree of ∧ and ∨ operations, you
could very reasonably trace the final result of the ∧/∨ back down the
tree to its origin, finding the non-CSG surface it originated from.
If you retained the computed values, you can do this in linear time in
the length of the path you trace, which is probably faster than
annotating each value with its origin on the way through this tree,
effectively applying the ∧/∨ operations to (*d*, id) pairs rather than
just *d* values.)

To me, one of the appealing aspects of successive-approximation
algorithms like SDF raymarching and any kind of Monte Carlo simulation
is that they can be used as “anytime algorithms” in order to guarantee
responsiveness at the expense, if necessary, of quality (see file
`anytime-realtime` and file `failure-free` for more on this).  My
current implementation does not do this.  How can I get this in
practice?

Using autodiff, you could perhaps “subsample” the SDF evaluations of
the final image, for example tracing a single ray through the center
of a 5×5 pixel square; at the final point of contact, rather than just
extracting the gradient of the SDF with respect to the (*x*, *y*, *z*)
scene coordinates, you could carry the autodiff computation all the
way through to the (*r*, *g*, *b*) color value and then compute its
Jacobian with respect to the (*u*, *v*) pixel coordinates on the
screen.  This Jacobian gives you a color gradient value at the center
of that 5×5 square, and so you could perhaps get an image that looks
like a badly compressed YouTube video frame, which is considerably
better than you would get by just downsampling an image from 640×360
to 128×72 (same number of samples) or 221×125 (same amount of data).

However, running on the CPU, you can *adaptively* subsample.  If you
reached the surface in a small number of SDF evaluations, you didn’t
pass close to any other surfaces, and the surface isn’t sharply angled
to the ray where you’re hitting it, so you can probably get by with
fewer samples.

Propagating SDF values to neighboring rays
------------------------------------------

Even without autodiff, computing an *exact* SDF *d*(*p⃗*) at some point
*p⃗* tells you a great deal about its values in the neighborhood of
*p⃗*.  At any displacement Δ*p⃗* in any direction, we know *d*(*p⃗*
+ Δ*p⃗*) ∈ [*d*(*p⃗*) - |Δ*p⃗*|, *d*(*p⃗*) + |Δ*p⃗*|], because the
highest it can be is if Δ*p⃗* follows the gradient away from the object
(and the gradient remains constant over that distance), and the lowest
it can be is in the opposite direction.  (No such pleasant bound holds
for inexact SDFs, neither the upper nor the lower bound.)

(Here by |·| I mean the Euclidean L₂ norm, as before.)

Still, though, even with an inexact SDF, we know that an entire sphere
of radius *d*(*p⃗*) around *p⃗* is devoid of, as they say, “geometry”.
This means that if you compute a non-tiny value from an *exact or
inexact* SDF for a ray shot from the camera, you have computed a bound
for a whole view frustum around that point; a single SDF evaluation is
enough to advance the whole wavefront around that point up to the
bound of a sphere around that point, at least the part of the
wavefront that has successfully *entered* that sphere.  It may be
worthwhile to use a conservative approximation of the sphere, such as
a ball made from a weighted sum of the L₁ norm (whose balls are
octahedra) and the L<sub>∞</sub> norm (whose balls are cubes).  I
think there is a cheap weighted-sum norm whose balls are these
irregular polyhedra with 32 triangular faces — “frequency-2 geodesic
spheres” in Buckminster Fuller’s terminology, except that the
underlying polyhedron whose triangular faces were subdivided into four
triangles is an octahedron, not a dodecahedron.

You can build a Z-buffer and pick arbitrary points in it and evaluate
the SDF at them; each SDF evaluation digs a big crater in the
neighboring Z-buffer values, but only those values that are initially
within the ball defined by that SDF value.

A fun way to do this might be to start with a very-low-resolution
Z-buffer (3×2, say), then repeatedly double its resolution, perhaps
using pairwise min to compute the newly interpolated pixels.  Each
doubling, you tighten the termination threshold for SDF iterations so
that it’s comparable to the new pixel resolution, and iterate over all
the Z-buffer pixels until you’ve hit that termination condition on
each of them.  This might be able to keep the total number of
(primary) SDF evaluations per pixel down to 1–3, instead of the 9.5
I’m seeing or the 100–1000 commonly seen in the demoscene.  If this
can be combined with the adaptive subsampling mentioned above, it
should be possible to get in the neighborhood of 0.1 SDF evaluations
per pixel.

I understand that on the GPU it’s dumb to try to do things like that
because communication between pixels is super expensive.  On the CPU,
though, it might be a sensible thing to do.

Multithreading and SIMD
-----------------------

On this three-torus scene, I’m getting 5.5 frames per second on my
laptop.

I’m not attempting to use SIMD operations like SSE and AVX, and the
algorithm is expressed in such a way (with data-dependent iteration
bailouts) that I would be very surprised if LuaJIT were finding a way
to take advantage of them.  I don’t think SSE 4.1 (which LuaJIT has)
or even SSE 4.2 (which my CPU has) support half-precision 16-bit
floats, so probably a 4× speedup for reasonably coherent vectors is
the most I can expect from SIMD.  (Again, see file `asus-gpu`.)

The CPU also has four cores, so in theory I should be able to get an
additional 4× speedup from multithreading, as long as the pixels
aren’t too interdependent.

So in theory I should be able to get a 16× speedup just by applying
brute force, working out to 88 fps (at 320×240).

Automated fabrication
---------------------

There’s another major reason I’m interested in SDFs that’s actually
not real-time graphics at all: automatic fabrication.  Historically
manufacturing has largely been concerned with the *geometry* of parts,
because steel, fired clay, concrete, glass, and polyethylene — the
best materials for many purposes — are sufficiently uniform and
isotropic that their geometry is most of what you need to know.  (Heat
treatment is important, but commonly applied to an entire article;
similarly painting, galvanizing, etc.  Surface finish is sometimes
important, but that’s in some sense a question of geometry too.  Work
hardening, for example from cold forging, is extremely important.)
Moreover, for metals in particular, merely achieving a desired
geometry is often a difficult and expensive proposition.

So algorithms that make contending with geometry tractable are of
great interest.

(I think one of the really interesting possibilities of automated
fabrication is actually that we can make things out of nonuniform and
anisotropic materials.)

The existing libraries, algorithms, and user interfaces for dealing
with 3-D geometry in computers are utter shit.  “Sketching”,
“lofting”, “extrusion”, “press/pull”, “pocketing”, “filleting”, and so
on, are terribly unexpressive; achieving even the most basic compound
curves is often beyond their capabilities.  The interactive sculpting
user interfaces in things like Blender and ZBrush are more expressive,
but incapable of handling any demands for precision.  The triangular
bounding surface meshes and NURBS commonly used as the internal data
representation are humongous, bug-prone, and — for any given level of
humongousness — terribly imprecise; furthermore, doing topological
optimization is basically impossible with them.  The available
libraries are buggy as shit and crash all the time.  Half the time
when you export a mesh from one program to import into another it
turns out not to be “manifold”, which is to say, it fails to represent
a set of solid objects. Voxel representations are, if anything, even
worse, but at least they can handle topopt and don’t have the
fucking “manifoldness” problem.

Christopher Olah’s “ImplicitCAD” is an attempt to remedy this
situation by using SDFs (and Haskell).  I’d like to play with the
approach and see what I can get working, but without Haskell.

So, what would it take to import an STL file into an SDF world?  How
about a thresholded voxel volumetric dataset?

SDFs in two dimensions
----------------------

Before I ever heard about 3-D raymarching using 3-D SDFs, I read a
now-lost Valve white paper about rendering text “decals” in games
using 2-D SDFs.  The idea is that you precompute a texture containing
a sampled SDF for the letterforms you want to use, and in your shader,
you sample from that texture, with the built-in bilinear interpolation
the GPU gives you.  Then, instead of just using the sampled value as a
pixel color, *you threshold it to get an alpha value*.  This allows
you to use the bilinear interpolation to interpolate sharp letterform
boundaries in between texels.

Moreover, by thresholding it *softly*, you can get antialiasing.  A
similar effect comes into play with 3-D SDFs if you stop marching the
ray when the SDF falls below about the scale of pixel spacing
projected on the surface: the SDF sphere smooths over surface detail
smaller than a pixel or so, preventing its high spatial frequencies
from aliasing down into lower frequencies on the screen.

A problem with this technique as described is that the bilinear
interpolation inevitably kind of rounds off sharp corners where you
would want them on the letterforms; to deal with this problem, you can
use two or more color channels to approximate different parts of the
letterform boundary with smooth curves, which cross at the desired
sharp corners.  There’s an open-source “mSDF” software package for
generating these “multichannel SDFs”.

I wonder if there’s a way to carry the analogy through to 3-D
raymarching with SDFs.  Perhaps, for example, it could somehow provide
a solution to my problem with rounding off edges.

Use in Dercuano
---------------

I’m pretty sure now that I can implement SDF-based sphere-tracing
raymarching in JS on `<canvas>` to get reasonable 3-D diagrams (see
file `dercuano-drawings` and file `dercuano-rendering`).  `<canvas>`
has a typed array interface for raw pixel access I used in Aikidraw,
so you don’t need a DOM call for every pixel you draw.  I think I can
render raster graphics into that interface pretty easily, fast enough
at least for still images.
