I started auditing an “operations research” class at the UBA this
quatrimester.  It’s largely focused on linear programming, which is to
say, optimizing linear functions subject to linear constraints, but
the class also covers some other non-linear-programming topics.

I’m particularly interested in optimization algorithms, as I’ve said a
few times before, because they potentially raise the level of
abstraction in programming — rather than specifying how to compute
something, you just specify how to recognize if something is the thing
you wanted to compute, and then let some kind of generic solver figure
out how to solve the problem.  That’s what earned them such a
prominent place in file `powerful-primitives`.

Overview of linear programming
------------------------------

Linear programming is a particularly popular subset of mathematical
optimization because there are well-known algorithms for solving it
that scale to fairly large numbers of decision variables
(the variables the optimization algorithm
is allowed to decide on a value for to reach the optimum), and it
provides a useful approximation for many economically important
problems.  It’s so popular that sometimes it overshadows the rest of
mathematical optimization — many books purportedly about mathematical
optimization consist almost entirely of material on linear
optimization, with perhaps a short section at the end on nonlinear
optimization.

I’ve tended to be a bit prejudiced against linear optimization because
the real world is never perfectly linear and because I felt that
linear constraints weren’t very expressive.  But, now that I’m taking
this class, I’m starting to appreciate the expressiveness of linear
programming — especially “mixed integer” linear programming, an
extension of pure linear programming in which some of your decision
variables are constrained to integer values.

As the lp\_solve documentation summarizes it:

> The linear programming (LP) problem can be formulated as: Solve **A**·*x* >=
 V₁, with V₂·*x* maximal. **A** is a matrix, *x* is a vector of (nonnegative)
 variables, V₁ is a vector called the right hand side, and V₂ is a
 vector specifying the objective function.

> An integer linear programming (ILP) problem is an LP with the
 constraint that all the variables are integers.  In a mixed integer
 linear programming (MILP) problem, some of the variables are integer
 and others are real.

There are several algorithms known for solving linear optimization
problems, of which the oldest and most-widely-used is George Dantzig’s
“simplex algorithm”, despite its worst-case exponential complexity.

The software landscape
----------------------

Typically, as an end-user, you solve linear optimization problems by
writing your problem in an “algebraic modeling language” such as GNU
MathProg or “GMPL” (a dialect of a popular proprietary modeling
language called “AMPL”) or ZIMPL; the model translator
compiles your model (including data files, which it might pull from a
database) into a standard format (such as “MPS”, “LP”, or `.nl` --- see below),
generally expanding it considerably, and submits it to a “solver”.

Unfortunately, the most popular model translators and
solvers are all proprietary; proprietary CPLEX, now owned by IBM, is
the most popular solver, with proprietary SCIP second, and proprietary
AMPL is the most popular model translator; there is an
excellent AMPL book by Brian Kernighan (et al., but “Kernighan” is a
sufficient recommendation) which also serves as an introduction to
linear programming.  Both CPLEX and SCIP are free for institutional
academic use but not to independent researchers; I don’t know about
AMPL.  So we can forget about these.

### Modeler–solver interfaces (MPS, Osi, `.nl`, and LP) ###

Most of the time the model translator talks to the solver through a file in
some standardized file format; MPS is the most popular such format, a
punched-card-based format originally designed for IBM’s Mathematical
Programming System/360.  LP, sometimes called “CPLEX LP” because of its
origins in CPLEX, is a more-readable but less-widely-supported format;
and AMPL, a popular but proprietary algebraic modeling language,
defined its own `.nl` format, mentioned earlier, that has been
interfaced to a wide variety of solvers.

Linear-programming solvers have been around considerably longer than
algebraic modeling languages, so both MPS and LP are designed for
humans to write, but MPS is designed badly.

(There are actually two different, incompatible MPS formats: the
original fixed-field MPS format from the 1960s, and a slightly less
bletcherous "free-form" 1980s version.)

Here's a small LP file that GLPK is able to accept and solve (the
optimum is z = 1, y = 0.33...).

    Maximize
     x: + 2 y + 3 z
    Subject To
     a: + 2 z + 3 y <= 3
    Bounds
     0 <= y <= 4
     0 <= z <= 1
    End

You can save that in `foo.lp` and solve it with GLPK by running
`glpsol --lp foo.lp -o foo.out`.  You can transate it to MPS with
`glpsol --lp foo.lp --wmps foo.mps`.

Some solvers, such as GLPK's, also have bindings that allow you to call
them from your program without preparing an input file for them to
parse; and the COIN-OR project has written a standard API called "Osi"
for doing this with many different solvers.

### GLPK (modeling language and solver) ###

GLPK, a C library, is the thousand-pound gorilla of linear
optimization systems; Octave is built with it, and there are bindings
to it in Java, Python, and R.  The GLPK package includes the
model translator for the popular
algebraic modeling language GNU MathProg (aka GMPL), and
also solvers for the revised simplex method, the primal-dual interior
point method, and the branch-and-bound method.

It supports MPS and LP format files for both input
and output, and can even translate between them, as well as generating
them with the GMPL model translator.

GLPK was originally released in 2000, but has been under active
development ever since; there is a Wikibook about it.

Here's a GMPL version of the example LP file above, with additional
directives to solve it and display the results:

    var y >= 0, <= 4;
    var z >= 0, <= 1;
    maximize x: 2*y + 3*z;
    subject to a: 2*z <= 3 - 3*y;
    solve;
    display x, y, z;
    end;

If you save this as `min.mod` you can run `glpsol -m min.mod` to get
the solution, or `glpsol -m min.mod --wlp min.lp` to get a slightly
expanded version of the LP file above.  Pitfall: if it's gzipped, like
most of the examples that come with GLPK, `glpsol` will uncompress it
to run it, but at least the version I have dies with a spurious "read
error".

This example is somewhat freer-form than the LP file (there's an
inequality with decision variables on both sides, for example), but to
really show the advantages of an algebraic modeling language like
GMPL, we need a bigger model.

### lp\_solve (solver) ###

lp\_solve is a simplex-method solver, using branch-and-bound for mixed
integer programming, which supports the MPS file format.  Early
versions were proprietary, but recent versions are free software.

I had some trouble getting it to interoperate with GLPK or ZIMPL.  I
*did* eventually get it to read an MPS file generated by ZIMPL, and I
think I know why the GLPK-generated version was failing; my example
problem (given in GMPL and LP format above) is a maximization problem,
not a minimization problem.  Upon translating it to MPS with `zimpl -t
mps min.zpl` (see below), ZIMPL issued the following warning:

    --- Warning: Objective function inverted to make
                 minimization problem for MPS output

And then lp\_solve was able to solve it with `lp_solve -mps min.mps`,
and indeed the value of the objective function was -3.66...7 rather
than 3.66...7.  So I guess that the problem was that MPS doesn't have
a way to say you're trying to maximize the objective function, not
minimize it.

With this in mind, I was then able to solve the free-form MPS file I
had generated with `glpsol -m min.mod --wfreemps min.fmps` by using
`lp_solve -max -fmps min.fmps`, `-max` being an `lp_solve` flag to
override this.  (I imagine the GLPK-generated fixed MPS file would
also have worked, but ZIMPL overwrote it.)

lp\_solve has its own "LP format" which is not the same as the CPLEX
LP format used by GLPK and ZIMPL, leading to much confusion on my
part.  The documentation does explain this:

> The lp-format is lpsolves native format to read and write lp
  models. Note that this format is not the same as the CPLEX LP format
  (see CPLEX lp files) or the Xpress LP format (see Xpress lp files)

I was able to translate the file from MPS to this incompatible LP
format with `lp_solve -max -fmps min.fmps -wlp min.lps.lp`:

    /* min */

    /* Objective function */
    max: +2 y +3 z;

    /* Constraints */
    a: +3 y +2 z <= 3;

    /* Variable bounds */
    y <= 4;
    z <= 1;

This can be reduced to the following, a format which is quite
reasonable for writing by hand, and still work as input to lp\_solve:

    max: +2 y +3 z;
    a: +3 y +2 z <= 3;
    y <= 4;
    z <= 1;

Unfortunately nothing but lp\_solve supports this format.  (But I
think it can translate it to MPS format.)

In theory lp\_solve supports CPLEX LP format with the flags `-rxli
xli_CPLEX` to read or `-wxli xli_CPLEX` to write, but I think the
Debian package failed to include the relevant module, so this doesn't
work.  (If you really needed to do this, you could use GLPK to
translate from LP to MPS (free MPS at least) so lp\_solve can handle
it.)

lp\_solve even supposedly has an "xli" to read GMPL, presumably linked
with GLPK.

Like GLPK, lp\_solve also has an API as well as file formats --- in C,
Java, and Free Pascal.

### ZIMPL (modeling language) ###

ZIMPL, the Zuse Institute Mathematical Programming Language,
is a modeling language from the same academic research
institute, the Zuse Institute Berlin, that wrote the solver SCIP, but
ZIMPL is free software, while SCIP is not.  It’s mostly compatible
with GNU MathProg.  It started out as Thorsten Koch’s Ph.D. thesis in
2004, ZIB-Report 04-58, “Rapid Mathematical Programming”.
(“Mathematical programming” is a synonym for “mathematical
optimization”.)

The ZIMPL language appears to be more or less at the same level of
abstraction as GMPL, but with arguably nicer syntax.
So for example in ZIMPL you
might say

    minimize cost: 12 * x1
        + sum a in A : u[a] * y[a];
    subto oneness: sum a in A : u[a] == 1;
    subto nonnegative: forall a in A : y[a] >= 0;

(though maybe you need `<a>` rather than `a` before `in`?) while in
AMPL/GMPL I think you would say

    minimize cost: 12 * x1
        + sum {a in A} u[a] * y[a];
    subject to Oneness: sum {a in A} u[a] = 1;
    subject to Nonnegative {a in A}: y[a] >= 0;

It can produce MPS and LP files, as well as something called
“Polynomial IP”.  Beware, it generates the name for its output file
from the name of the input file, and so it will overwrite any existing
file of the corresponding name.

Here's a translation of the tiny model used as examples earlier into
ZIMPL:

    var y >= 0;
    var z >= 0;
    maximize x: 2*y + 3*z;
    subto a: 2*z <= 3 - 3*y;
    subto ym: y <= 4;
    subto zm: z <= 1;

If saved as `foo.zpl`, you can convert it to LP format with `zimpl
foo.zpl`, writing the output to `foo.lp`.

Since ZIMPL and GMPL are so similar in their capabilities, but GMPL is
bundled with the popular GLPK solver, I don't know what the advantage
of using ZIMPL would be.

### libisl (ILP solver) ###

The documentation says:

> isl is a library for manipulating sets and relations of integer points
 bounded by linear constraints. Supported operations on sets include
 intersection, union, set difference, emptiness check, convex hull,
 (integer) affine hull, integer projection, and computing the lexicographic
 minimum using parametric integer programming. It also includes an ILP solver
 based on generalized basis reduction.

I have it on my laptop because GCC depends on it, apparently for a
newish loop optimization framework internal to GCC called Graphite.
It sounds scarily powerful.  Unlike the others, it's specifically
intended for *integer* optimization, with no support for
continuous-domain optimization.

Needless to say, it doesn't interoperate with the others.

### COIN CLP/CBC/SYMPHONY (solvers) ###

The COIN project is an Apache project for making operations-research
results reproducible by basing them on open-source software.  It
includes the solvers CLP, CBC, and SYMPHONY, which last requires an
underlying linear-programming solver like CLP.  I haven't yet figured
out how to use any of them.

### NLopt (solver) ###

NLopt is a *non*linear optimization library, so it probably doesn’t
really belong in this list, but in theory it should be able to solve
linear optimization problems too, as a special case; it will be
interesting to compare it.  It has bindings in C, C++, Fortran,
Octave, Python, Guile, and R.

### Octave (programming environment) ###

Although Octave is mostly a matrix computation system, it includes
support for linear programming, as well as quadratic programming,
general nonlinear programming, and linear least-squares solution of
matrices.

### Pyomo (modeling embedded DSL) ###

Pyomo, formerly "Coopr", is an algebraic modeling language like ZIMPL
or GMPL, but realized as an embedded DSL in Python, or perhaps more
accurately, a Python API, with the concepts closely matching those of
ZIMPL or GMPL.  It supports linear optimization and also quadratic
optimization, general nonlinear optimization, etc.  It's able to read
model parameters from data files in GMPL format, but not to read GMPL
models.

Pyomo can use GLPK, CBC, or some undocumented set of other solvers.

### FLOPC++ (modeling embedded DSL) ###

FLOPC++ --- another part of the COIN-OR project --- is, similarly, an
algebraic modeling language realized as an embedded DSL in C++.  I
haven't tried it but it looks like roughly the same level of pain as
Pyomo.  It can at least use CBC.

Hacks to expand what you can do with a linear optimizer
-------------------------------------------------------

The description from lp\_solve makes linear optimization sound really
weak and limiting:

> Solve **A**·*x⃗* >= V⃗₁, with V⃗₂·*x⃗* maximal.

And, in some ways, it is; but as I’m learning in this class, there are
standard hacks for turning a much larger variety of problems into
linear problems.  Some of these are done for you automatically by
model translators for algebraic modeling languages, while others
aren't.

### Minimizing vs. maximizing ###

The simplest hack is switching between minimizing and maximizing.  The
lp\_solve definition above might suggest that you can’t minimize
things, but of course when *x⃗* is chosen to make V⃗₂·*x⃗* minimal, then
(-1 · V⃗₂) · *x⃗* is maximized.  Moreover, its maximal value is
precisely the opposite of the minimal value for V⃗₂·*x⃗*; so if your
solver wants to maximize, you can just flip the sign on its objective
vector and the objective result, and you can minimize.

As mentioned earlier, ZIMPL will do this when generating an MPS file,
since apparently MPS can't express whether you want to maximize or
minimize your objective function, and so lp\_solve assumes you want to
minimize.

### Mixing >= with <= ###

Similarly, **A**·*x⃗* >= V⃗₁ suggests that all your constraints must
have decision variables on the left side of a “>=”.  But that’s silly;
the constraint 2*y* >= 4, for example, is equivalent to -2*y* <= -4.
So by flipping the signs on a row of the matrix and the corresponding
limit, you can effectively get both <= and >= constraints in the same
matrix.  And that’s how you can get criteria like 1 <= 2*y* + 3*z*
<= 2.

Indeed, even the CPLEX LP format supports this directly for decision
variables:

    Bounds
     0 <= y <= 4
     0 <= z <= 1

### Decision variables on both sides ###

Similarly, if you have a criterion like 2*y* <= 3*z* + 1, you might
think that doesn’t fit into the **A**·*x⃗* >= V⃗₁ mold.  But all you
have to do is subtract 3*z* from both sides — 2*y* - 3*z* <=
1 — and Bob’s your auntie!

### Equality constraints ###

Similarly, if you have an *equality* criterion like 3*p* = 4*q* -
2*r*, you can convert it into two *inequalities* — 3*p* <= 4*q* -
2*r* and 3*p* >= 4*q* - 2*r* — which can only be simultaneously
true when strict equality is satisfied.  This reduces the
dimensionality of your feasible region below the full dimensionality
of the decision-variable space, though that’s not a problem for the
simplex method.

### Binary gates ###

### Absolute values ###

### Piecewise-linear functions ###
